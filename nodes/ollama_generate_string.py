import os
import json
from collections import deque
 
from server import PromptServer 
from aiohttp import web
import aiohttp
import asyncio

OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "127.0.0.1:11434")


"""

    To be able to store the generated LLM prompt into the png for reproducibility the flow is as follows:

    This node takes:
        generate_seed:      This is the seed used for the next generation.
        generate_prompt:    This is the prompt used for the next generation.
        use_seed            This is the seed used for the current llm result used as output artifact (generated in previous invocation)
        use_result          This is the result generated by the previous invocation, used for the image generation.
        use_prompt          This is the prompt used by the previous invocation, used for the image generation.
    
    When executed:
        generate_seed + generate_prompt is used to generate an LLM response, this is sent to the UI.
        The ui stores use_* and sends it to the node for image generation.
        
    This way the use_* coming in is stored in the generated image.
"""

class OllamaGenerateString :
    def __init__(self): 
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "generate": ("BOOLEAN", {"default": True, "label_on": "Generating New", "label_off": "Reusing", "tooltip": "Whether to cycle the result, or keep it fixed to the currently shown value."}),
                "generate_seed": ("INT",{"defaultInput": True}),
                "generate_prompt": ("STRING",{"defaultInput": True, "default": "", "multiline": True, "dynamicPrompts": False}),
                # And the actually used values follow;
                "use_seed": ("INT",{"defaultInput": False, "disabled": True, "default":"",}),
                "use_result": ("STRING",{"defaultInput": False, "multiline": True, "default":"",}), 
                "use_prompt": ("STRING",{"defaultInput": False, "multiline": True, "default":"",}), 
            },
            "optional": {
            }, 
        }

    RETURN_TYPES = ("STRING", )
    FUNCTION = "execute"
    
    # Do we need this?
    # OUTPUT_IS_LIST = (False,)

    CATEGORY = "iw"

    @classmethod
    def IS_CHANGED(cls, generate_seed, generate_prompt, use_seed=None, use_result=None, use_prompt=None, generate=True): 
        return "foo"


    def execute(self, generate_seed, generate_prompt, use_seed=None, use_result=None, use_prompt=None, generate=True): 
        result = (use_result,) 
        print(f"executing with {generate} {generate_seed} {generate_prompt}")
        print(f"      Using    {use_seed} {use_result} {use_prompt}")
        
     
        if generate: 
            next_seed = generate_seed
            next_result = f"generated with {generate_prompt} and seed {generate_seed}"
            next_prompt = generate_prompt
        else:
            next_seed = use_seed
            next_result = use_result
            next_prompt = use_prompt

        return {
            "ui": {
                    "next_seed": [next_seed,],
                    "next_result": [next_result,],
                    "next_prompt": [next_prompt,],
                    },
            "result": result
        } 

@PromptServer.instance.routes.get("/iw/api/ollama/models")
async def retrieve_models(request): 
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f'http://{OLLAMA_HOST}/api/tags') as response:
                body = await response.json()
                model_simple = [v["model"] for v in body["models"]]
                return web.json_response({"status": "success", "models": model_simple}, status=200)
    except aiohttp.client_exceptions.ClientConnectorError as e:
        return web.json_response({"status": "success", "models": ["ollama is down"]}, status=200)
        

    return web.json_response({"status": "fail"}, status=500)
