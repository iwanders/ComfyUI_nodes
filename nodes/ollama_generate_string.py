import os
import json
from collections import deque
 
from server import PromptServer 
from aiohttp import web
import aiohttp
import asyncio
import requests

OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "127.0.0.1:11434")


"""

    To be able to store the generated LLM prompt into the png for reproducibility the flow is as follows:

    This node takes:
        generate_seed:      This is the seed used for the next generation.
        generate_prompt:    This is the prompt used for the next generation.
        use_seed            This is the seed used for the current llm result used as output artifact (generated in previous invocation)
        use_result          This is the result generated by the previous invocation, used for the image generation.
        use_prompt          This is the prompt used by the previous invocation, used for the image generation.
    
    When executed:
        generate_seed + generate_prompt is used to generate an LLM response, this is sent to the UI.
        The ui stores use_* and sends it to the node for image generation.
        
    This way the use_* coming in is stored in the generated image.
"""


def get_model_list():
    try:
        response = requests.get(f'http://{OLLAMA_HOST}/api/tags')
        body = response.json()
        return [v["model"] for v in body["models"]]
    except requests.exceptions.ConnectionError as e:
        return []

MODEL_LIST = get_model_list()


class OllamaGenerateString :
    def __init__(self): 
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "generate": ("BOOLEAN", {"default": True, "label_on": "Generating New", "label_off": "Reusing", "tooltip": "Whether to cycle the result, or keep it fixed to the currently shown value."}),

                "generate_seed": ("INT",{"defaultInput": True}),
                "generate_prompt": ("STRING",{"defaultInput": True, "default": "", "multiline": True, "dynamicPrompts": False}),
                "generate_model": (MODEL_LIST, {"default":MODEL_LIST[0] if MODEL_LIST else ""}),
                # And the actually used values follow;
                "use_seed": ("INT", {"defaultInput": False, "disabled": True, "default":0,}),
                "use_result": ("STRING",{"defaultInput": False, "multiline": True, "default":"",}), 
                "use_prompt": ("STRING",{"defaultInput": False, "multiline": True, "default":"",}), 
 
            },
            "optional": {
            }, 
        }

    RETURN_TYPES = ("STRING", )
    FUNCTION = "execute"
    
    # Do we need this?
    # OUTPUT_IS_LIST = (False,)

    CATEGORY = "iw"

    @classmethod
    def IS_CHANGED(cls,generate_model, generate_seed, generate_prompt, use_seed=None, use_result=None, use_prompt=None, generate=True): 
        return "foo"


    async def execute(self,generate_model, generate_seed, generate_prompt, use_seed=None, use_result=None, use_prompt=None, generate=True): 
        result = (use_result,) 
        print(f"executing with {generate_model} {generate} {generate_seed} {generate_prompt}")
        print(f"      Using    {use_seed} {use_result} {use_prompt}")
        
     
        if generate and MODEL_LIST:
            """
                curl http://localhost:11434/api/generate -d '{
                  "model": "gemma3",
                  "prompt": "Why is the sky blue?"
                }'
            """

            """
            async def request(): 
                url = "https://httpbin.org/post" # An endpoint that echoes POST data
                payload = {"model": "value", "example": "test"}
                headers = {"Content-Type": "application/json"} # Optional: specify content type

                # Use a ClientSession as an async context manager to manage connections
                async with aiohttp.ClientSession(headers=headers) as session:
                    # Perform the POST request asynchronously and await the response
                    async with session.post(url, json=payload) as response:
                        # Await the response data (e.g., as JSON)
                        print(f"Status: {response.status}")
                        response_json = await response.json()
                        print("Response JSON:")
                        print(json.dumps(response_json, indent=2))
         
            asyncio.run(main())
            """
            next_seed = generate_seed
            next_result = f"generated with {generate_prompt} and seed {generate_seed}"
            next_prompt = generate_prompt
        else:
            next_seed = use_seed
            next_result = use_result
            next_prompt = use_prompt

        return {
            "ui": {
                    "next_seed": [next_seed,],
                    "next_result": [next_result,],
                    "next_prompt": [next_prompt,],
                    },
            "result": result
        } 

@PromptServer.instance.routes.get("/iw/api/ollama/models")
async def retrieve_models(request):
    return web.json_response({"status": "success", "models": MODEL_LIST}, status=200)
