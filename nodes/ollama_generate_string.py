import os
import json
from collections import deque
 
from server import PromptServer 
from aiohttp import web
import aiohttp
import asyncio
import requests
import time

OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "127.0.0.1:11434")


"""

    To be able to store the generated LLM prompt into the png for reproducibility the flow is as follows:

    This node takes:
        generate_seed:      This is the seed used for the next generation.
        generate_prompt:    This is the prompt used for the next generation.
        use_seed            This is the seed used for the current llm result used as output artifact (generated in previous invocation)
        use_result          This is the result generated by the previous invocation, used for the image generation.
        use_prompt          This is the prompt used by the previous invocation, used for the image generation.
    
    When executed:
        generate_seed + generate_prompt is used to generate an LLM response, this is sent to the UI.
        The ui stores use_* and sends it to the node for image generation.
        
    This way the use_* coming in is stored in the generated image.
"""


def get_model_list():
    try:
        response = requests.get(f'http://{OLLAMA_HOST}/api/tags')
        body = response.json()
        return [v["model"] for v in body["models"]]
    except requests.exceptions.ConnectionError as e:
        return []

MODEL_LIST = get_model_list()


from collections import OrderedDict

# Dict with upper bound, made by the llm.
class MaxSizeDict(OrderedDict):
    def __init__(self, max_size, *args, **kwargs):
        self.max_size = max_size
        super().__init__(*args, **kwargs)

    def __setitem__(self, key, value):
        super().__setitem__(key, value)
        if self.max_size > 0 and len(self) > self.max_size:
            # Remove the oldest item (popitem(False) removes the first item added)
            self.popitem(last=False)


class OllamaGenerateString :
    def __init__(self): 
        self._cache = MaxSizeDict(10)
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "generate": ("BOOLEAN", {"default": True, "label_on": "Generating New", "label_off": "Reusing", "tooltip": "Whether to cycle the result, or keep it fixed to the currently shown value."}),

                "generate_seed": ("INT",{"defaultInput": True}),
                "generate_prompt": ("STRING",{"defaultInput": True, "default": "", "multiline": True, "dynamicPrompts": False}),
                "generate_model": (MODEL_LIST, {"default":MODEL_LIST[0] if MODEL_LIST else ""}),
                # And the actually used values follow;
                "use_seed": ("INT", {"defaultInput": False, "disabled": True, "default":0,}),
                "use_result": ("STRING",{"defaultInput": False, "multiline": True, "default":"",}), 
                "use_prompt": ("STRING",{"defaultInput": False, "multiline": True, "default":"",}), 
 
            },
            "optional": {
            }, 
        }

    RETURN_TYPES = ("STRING", )
    FUNCTION = "execute"
    
    # Do we need this?
    # OUTPUT_IS_LIST = (False,)

    CATEGORY = "iw"

    @classmethod
    def IS_CHANGED(cls, generate_model, generate_seed, generate_prompt, use_seed=None, use_result=None, use_prompt=None, generate=True): 
        """
            Despite the name, IS_CHANGED should not return a bool
            IS_CHANGED is passed the same arguments as the main function defined by FUNCTION,
            and can return any Python object. This object is compared with the one returned in
            the previous run (if any) and the node will be considered to have changed if
            is_changed != is_changed_old (this code is in execution.py if you need to dig).
        """
        #print(f"IS_CHANGED executing with {generate_model} {generate} {repr(generate_seed)} {generate_prompt}")
        #print(f"IS_CHANGED       Using    {repr(use_seed)}    {use_prompt}   {len(use_result)} ")
        #if generate:
        #    # User can still modify these values from the UI, so lets just put them here.
        #    return time.time();

        # we are generating, if seed & prompt is the same, we don't need to run.
        # additionally... what's worse is that if generate_seed is a separate node, it is always None :<
        # So we can't do this smart check here...
        #if use_seed == generate_seed and use_prompt == generate_prompt:
        #    print("Nothing to do" )
        #    return False

        # Otherwise... lets just return the values as we'd currently get from the UI... 
        return f"{use_result} {use_seed} {use_prompt}"


    async def execute(self,generate_model, generate_seed, generate_prompt, use_seed=None, use_result=None, use_prompt=None, generate=True): 
        #print(f"executing with {generate_model} {generate} {generate_seed} {generate_prompt}")
        #print(f"      Using    {use_seed} {use_prompt}  res is {len(use_result)} ")
        
     
        if generate and MODEL_LIST:
            next_seed = generate_seed
            next_prompt = generate_prompt

            # Since IS_CHANGED above is super clunky and doesn't get the seed, do this quick cache thing here.
            key = (generate_seed, generate_model, generate_prompt)

            # if we don't have this in the cache, actually run the model and cache it.
            if not key in self._cache:
                payload = {"model": generate_model, "prompt": generate_prompt, "stream": False, "options": {"seed": generate_seed}}
                headers = {"Content-Type": "application/json"} 
     
                async with aiohttp.ClientSession(headers=headers) as session: 
                    async with session.post(f"http://{OLLAMA_HOST}/api/generate", json=payload) as response:
                        response_json = await response.json() #content_type=None 
                        self._cache[key] = response_json["response"]

            next_result = self._cache[key]
        else:
            # Using data from the UI as-is.
            next_seed = use_seed
            next_result = use_result
            next_prompt = use_prompt

        return {
            "ui": {
                    "next_seed": [next_seed,],
                    "next_result": [next_result,],
                    "next_prompt": [next_prompt,],
                    },
            "result": [use_result,],
        } 

@PromptServer.instance.routes.get("/iw/api/ollama/models")
async def retrieve_models(request):
    return web.json_response({"status": "success", "models": MODEL_LIST}, status=200)
